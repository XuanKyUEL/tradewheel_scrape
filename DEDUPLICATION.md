# URL-Based Deduplication Feature

## Overview

The Tradewheel Scraper now includes automatic deduplication functionality that removes duplicate entries based on the URL column. This ensures that each scraping run only saves new, unique leads that haven't been collected before.

## How It Works

### 1. Load Previous URLs
When the scraper runs, it:
- Scans all existing CSV files in the `data/` directory
- Extracts and stores all unique URLs from previous scraping sessions
- Creates a set of known URLs to check against

### 2. Deduplicate New Data
After scraping new pages, the scraper:
- Compares each newly scraped URL against the set of previous URLs
- Removes any entries that match existing URLs
- Also prevents intra-batch duplicates (duplicates within the same scraping session)

### 3. Save Only Unique Data
Finally, the scraper:
- Saves only the unique, non-duplicate entries to CSV and Excel files
- Reports the number of duplicates removed
- Reports the final count of unique leads saved

## Usage

The deduplication feature is **automatically enabled** and requires no configuration. It runs automatically during every scraping session.

### Process Flow

```
1. Start scraping â†’ Scrape pages 1-15
2. Collect all leads â†’ 300 leads found
3. Load previous URLs â†’ 1722 URLs from previous files
4. Deduplicate â†’ Remove 150 duplicates
5. Save results â†’ Save 150 unique new leads
```

## Example Output

```
ğŸš€ Báº¯t Ä‘áº§u scrape tá»« trang 1 Ä‘áº¿n 15
...
ğŸ“‚ Found 11 previous CSV files
âœ… Loaded 1722 unique URLs from previous data
ğŸ” Deduplication: 300 â†’ 150 (150 duplicates removed)
ğŸ“Š Unique leads saved: 150
```

## Technical Details

### Key Functions

#### `load_previous_urls()`
- **Purpose**: Load all URLs from previous CSV files
- **Returns**: Set of unique URLs
- **Location**: `src/scraper.py`

```python
previous_urls = scraper.load_previous_urls()
# Returns: {'https://www.tradewheel.com/buyers/...', ...}
```

#### `deduplicate_data(previous_urls=None)`
- **Purpose**: Remove duplicate entries based on URL
- **Parameters**: 
  - `previous_urls` (optional): Pre-loaded set of URLs. If None, loads automatically
- **Returns**: Number of duplicates removed
- **Location**: `src/scraper.py`

```python
duplicates_removed = scraper.deduplicate_data()
# Returns: 150 (number of duplicates removed)
```

### Integration in Workflow

The deduplication is integrated into the main `run()` method:

```python
# Scrape data
total_leads = self.scrape_all_pages(start_page, end_page)

# Deduplicate against previous data
self.deduplicate_data()

# Check if any unique data remains
if len(self.results) == 0:
    print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u má»›i sau khi loáº¡i bá» duplicate!")
    return False, None, 0

# Save unique data to files
self.save_to_csv(csv_file)
```

## Testing

### Unit Tests
Run the deduplication tests:
```bash
python test_deduplication.py
```

Tests include:
- âœ… Load previous URLs from CSV files
- âœ… Deduplicate data against previous URLs
- âœ… Handle intra-batch duplicates
- âœ… Preserve only unique entries

### Verification with Actual Data
Verify deduplication with actual repository data:
```bash
python verify_deduplication.py
```

## Benefits

1. **No Duplicate Storage**: Saves disk space by avoiding duplicate records
2. **Data Quality**: Maintains clean, unique dataset across all scraping sessions
3. **Efficient**: Only new leads are saved in each run
4. **Automatic**: No manual intervention required
5. **Git-Friendly**: Smaller commits with only new data

## Edge Cases Handled

- **No previous data**: If this is the first run, no URLs are loaded and all data is considered new
- **Empty results**: If all scraped data is duplicate, the scraper reports this and doesn't create empty files
- **Malformed URLs**: Entries with missing or invalid URLs are handled gracefully
- **File read errors**: If a previous CSV file can't be read, it's skipped with a warning

## Statistics (Example Run)

From repository data analysis:
- **Total CSV files**: 11
- **Total unique URLs**: 1,722
- **Average URLs per file**: ~156
- **Deduplication effectiveness**: Typically removes 40-50% of newly scraped data

## Notes

- Deduplication is based **only on the URL column**
- Other fields (title, description, etc.) are not used for comparison
- The feature works across all CSV files in the `data/` directory
- No historical data is modified - only new data is filtered
