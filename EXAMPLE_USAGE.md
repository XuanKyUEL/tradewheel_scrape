# Example: URL-Based Deduplication in Action

This document shows a practical example of how the URL-based deduplication feature works.

## Scenario

You have already scraped Tradewheel data several times, and your `data/` directory contains:

```
data/
â”œâ”€â”€ 08_15_25_tradewheel_scrap.csv  (199 leads)
â”œâ”€â”€ 09_01_25_tradewheel_scrap.csv  (198 leads)
â”œâ”€â”€ 09_03_25_tradewheel_scrap.csv  (199 leads)
â”œâ”€â”€ 09_15_25_tradewheel_scrap.csv  (199 leads)
â”œâ”€â”€ 09_16_25_tradewheel_scrap.csv  (199 leads)
â”œâ”€â”€ 09_17_25_tradewheel_scrap.csv  (196 leads)
â”œâ”€â”€ 09_29_25_tradewheel_scrap.csv  (199 leads)
â”œâ”€â”€ 10_20_25_tradewheel_scrap.csv  (20 leads)
â”œâ”€â”€ 10_29_25_tradewheel_scrap.csv  (199 leads)
â”œâ”€â”€ 11_01_25_tradewheel_scrap.csv  (198 leads)
â””â”€â”€ 11_15_25_tradewheel_scrap.csv  (20 leads)

Total: 1,722 unique URLs
```

## Running a New Scrape

When you run the scraper on November 19, 2025:

```bash
python src/main.py
```

### Step 1: Scraping Pages
```
ğŸš€ Báº¯t Ä‘áº§u scrape tá»« trang 1 Ä‘áº¿n 15
ğŸ“„ Äang scrape trang 1 - https://www.tradewheel.com/buyers/?page=1
âœ… TÃ¬m tháº¥y 20 leads má»›i
ğŸ“„ Äang scrape trang 2 - https://www.tradewheel.com/buyers/?page=2
âœ… TÃ¬m tháº¥y 20 leads má»›i
...
ğŸ“„ Äang scrape trang 15 - https://www.tradewheel.com/buyers/?page=15
âœ… TÃ¬m tháº¥y 20 leads má»›i
ğŸ“Š Tá»•ng cá»™ng: 300 leads tá»« 300 records
```

### Step 2: Loading Previous URLs
```
ğŸ“‚ Found 11 previous CSV files
âœ… Loaded 1722 unique URLs from previous data
```

The scraper reads all 11 CSV files and builds a set of 1,722 unique URLs that have been scraped before.

### Step 3: Deduplication
```
ğŸ” Deduplication: 300 â†’ 145 (155 duplicates removed)
```

Out of the 300 newly scraped leads:
- **155 URLs** already exist in previous files â†’ **Removed**
- **145 URLs** are new and unique â†’ **Kept**

### Step 4: Saving Results
```
ğŸ’¾ Äang lÆ°u CSV: data/11_19_25_tradewheel_scrap.csv
âœ… ÄÃ£ lÆ°u 145 records vÃ o CSV
ğŸ”„ Äang convert sang Excel: data/11_19_25_tradewheel_scrap.xlsx
âœ… ÄÃ£ convert thÃ nh cÃ´ng sang Excel

ğŸ‰ HOÃ€N THÃ€NH!
ğŸ“ File Excel: data/11_19_25_tradewheel_scrap.xlsx
ğŸ“Š Unique leads saved: 145
```

## Result

Your `data/` directory now contains:

```
data/
â”œâ”€â”€ ... (previous files)
â”œâ”€â”€ 11_15_25_tradewheel_scrap.csv  (20 leads)
â””â”€â”€ 11_19_25_tradewheel_scrap.csv  (145 leads) â† NEW FILE with only unique data!

Total: 1,867 unique URLs (1,722 + 145)
```

## Benefits Demonstrated

1. **Space Savings**: Saved 51.7% storage by not storing 155 duplicate entries
2. **Data Quality**: Maintained clean dataset with no duplicates
3. **Efficiency**: Only new data is saved
4. **Automatic**: No manual intervention needed

## What Gets Deduplicated?

### Example Duplicate (Removed)
```csv
"USA","14 Nov, 2025","Looking for Lentils","https://www.tradewheel.com/buyers/looking-for-lentils/900673/","Hello, I need Lentils","2025-11-19 10:00:00"
```
This URL already exists in `11_15_25_tradewheel_scrap.csv`, so it's **removed**.

### Example New Entry (Kept)
```csv
"Germany","19 Nov, 2025","Importing Steel Pipes","https://www.tradewheel.com/buyers/importing-steel-pipes/900800/","Need steel pipes for construction","2025-11-19 10:00:00"
```
This URL is new and doesn't exist in any previous file, so it's **kept**.

## Edge Cases Handled

### Case 1: First Run (No Previous Data)
```
â„¹ï¸ No previous CSV files found
ğŸ” Deduplication: 300 â†’ 300 (0 duplicates removed)
```
All 300 leads are saved because this is the first scraping run.

### Case 2: All Duplicates
```
ğŸ“‚ Found 11 previous CSV files
âœ… Loaded 1722 unique URLs from previous data
ğŸ” Deduplication: 300 â†’ 0 (300 duplicates removed)
âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u má»›i sau khi loáº¡i bá» duplicate!
```
No new file is created because all scraped data already exists.

### Case 3: Intra-Batch Duplicates
```
ğŸ“„ Äang scrape trang 1 - (finds URL A)
ğŸ“„ Äang scrape trang 2 - (finds URL A again)
```
The duplicate within the same batch is also removed, ensuring each URL appears only once.

## Verification

You can verify the deduplication is working:

```bash
# Run the verification script
python verify_deduplication.py

# Check unique URLs in all files
find data/*.csv -exec wc -l {} \; | awk '{sum+=$1} END {print sum}'
```

## Summary

The deduplication feature ensures that:
- âœ… Every URL is stored only once across all CSV files
- âœ… Storage is optimized by not saving duplicates
- âœ… Data quality is maintained automatically
- âœ… Git commits only contain new, unique data
