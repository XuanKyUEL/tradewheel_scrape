# ğŸ•·ï¸ Tradewheel Auto Scraper

**Automated scraping solution for Tradewheel.com with GitHub Actions scheduling**

## ğŸ¯ Features

- âœ… **Auto scraping every 2 weeks** via GitHub Actions
- âœ… **Automatic Excel export** with formatting
- âœ… **Data stored in repository** with version control
- âœ… **Anti-detection measures** (random delays, proper headers)
- âœ… **URL-based deduplication** - automatically removes duplicates from previous scrapes
- âœ… **Duplicate removal** and data cleaning
- âœ… **Manual triggering** support
- âœ… **Local development** support

## ğŸ“ Project Structure

```
tradewheel-scraper/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ scrape-tradewheel.yml    # GitHub Actions workflow
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                      # Entry point for GitHub Actions
â”‚   â”œâ”€â”€ scraper.py                   # Main scraper module
â”‚   â”œâ”€â”€ config_github.py             # Production config
â”‚   â””â”€â”€ __init__.py                  # Package init
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_local.py                 # Local development script
â”œâ”€â”€ data/                            # Auto-generated data files
â”‚   â”œâ”€â”€ MM_DD_YY_tradewheel_scrap.xlsx
â”‚   â””â”€â”€ MM_DD_YY_tradewheel_scrap.csv
â”œâ”€â”€ config.py                        # Local config (gitignored)
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .gitignore                       # Git ignore rules
â””â”€â”€ README.md                        # This file
```

## ğŸš€ Setup & Usage

### 1. GitHub Repository Setup

1. **Fork/Clone this repository**
2. **Enable GitHub Actions** in repository settings
3. **Repository will auto-scrape every 2 weeks**

### 2. Manual Triggering

Go to **Actions** tab â†’ **Tradewheel Auto Scraper** â†’ **Run workflow**

Options:

- `start_page`: Starting page (default: 1)
- `end_page`: Ending page (default: 15)

### 3. Local Development

```bash
# Install dependencies
pip install -r requirements.txt

# Run local test (5 pages)
python scripts/run_local.py

# Run with custom config
python src/main.py
```

### 4. Schedule Configuration

Edit `.github/workflows/scrape-tradewheel.yml`:

```yaml
schedule:
  # Every 2 weeks on Monday at 9:00 AM UTC
  - cron: "0 9 */14 * 1"

  # Other examples:
  # - cron: '0 9 * * 1'      # Every Monday
  # - cron: '0 9 */7 * *'    # Every week
  # - cron: '0 9 1 * *'      # Monthly on 1st
```

## ğŸ“Š Data Output

### Auto-generated files in `/data/`:

- `MM_DD_YY_tradewheel_scrap.xlsx` - Formatted Excel file
- `MM_DD_YY_tradewheel_scrap.csv` - Raw CSV data

### Data Structure:

| Column      | Description              |
| ----------- | ------------------------ |
| country     | Buyer's country          |
| date_posted | When the lead was posted |
| title       | Lead title/heading       |
| url         | Direct URL to the lead   |
| bdesc       | Cleaned description      |
| crawl_time  | When data was scraped    |

### Deduplication:

The scraper **automatically removes duplicates** based on the URL column:

- ğŸ” Scans all previous CSV files in `/data/` directory
- ğŸ—‘ï¸ Removes entries with URLs that already exist
- âœ¨ Saves only new, unique leads in each run
- ğŸ“Š Reports deduplication statistics

For more details, see [DEDUPLICATION.md](DEDUPLICATION.md)

## âš™ï¸ Configuration

### GitHub Actions Config (`src/config_github.py`):

```python
SCRAPING_CONFIG = {
    "start_page": 1,
    "end_page": 15,          # Adjust pages here
    "min_delay": 3,          # Delay between pages
    "max_delay": 7,
}
```

### Local Config (`config.py`):

Create `config.py` for local development:

```python
CHROME_CONFIG = {
    "binary_location": r"C:\Program Files\Google\Chrome\Application\chrome.exe",
    "chromedriver_path": r"C:\path\to\chromedriver.exe",
}
```

## ğŸ¤– GitHub Actions Workflow

The automation includes:

1. **Scheduled runs** every 2 weeks
2. **Chrome/ChromeDriver installation** on Ubuntu
3. **Data scraping** with error handling
4. **Auto-commit** results to repository
5. **Artifact upload** for backup

## ğŸ”§ Customization

### Change Schedule:

Edit the cron expression in workflow file:

```yaml
# Every 2 weeks (current)
- cron: "0 9 */14 * 1"

# Weekly
- cron: "0 9 * * 1"

# Monthly
- cron: "0 9 1 * *"
```

### Change Page Range:

1. **For automation**: Edit `config_github.py`
2. **For manual runs**: Use workflow inputs
3. **For local**: Edit `run_local.py`

### Data Storage:

Files are automatically committed to `/data/` folder with:

- Timestamp in filename
- Git version control
- 90-day artifact retention

## ğŸš¨ Monitoring

### Check automation status:

1. Go to **Actions** tab
2. View workflow run history
3. Check logs for errors
4. Download artifacts if needed

### Troubleshooting:

- **Workflow fails**: Check Actions logs
- **No data**: Website might be down/changed
- **Local issues**: Check Chrome/ChromeDriver paths

## ğŸ“ˆ Data History

All scraped data is preserved in the repository with:

- âœ… Git commit history
- âœ… Timestamped files
- âœ… Full audit trail
- âœ… Easy data comparison

## ğŸ”’ Security

- No sensitive data stored
- Public repository safe
- Uses GitHub's secure runners
- No API keys required

---

**ğŸ‰ Set up once, data flows automatically every 2 weeks!**

- âœ… Configurable parameters
- âœ… Error handling

## Troubleshooting

### Lá»—i Chrome binary

Náº¿u gáº·p lá»—i Chrome binary, hÃ£y kiá»ƒm tra Ä‘Æ°á»ng dáº«n trong `config.py`:

```python
"binary_location": r"C:\Program Files\Google\Chrome\Application\chrome.exe"
```

### Lá»—i chromedriver

Kiá»ƒm tra Ä‘Æ°á»ng dáº«n chromedriver trong `config.py`:

```python
"chromedriver_path": r"E:\advantage_scrapping\chromedriver-win64\chromedriver.exe"
```
