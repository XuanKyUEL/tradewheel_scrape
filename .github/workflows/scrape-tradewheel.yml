name: Tradewheel Auto Scraper

on:
  schedule:
    # Ch·∫°y m·ªói 2 tu·∫ßn v√†o th·ª© 2 l√∫c 9:00 AM UTC (4:00 PM GMT+7)
    - cron: "0 9 */14 * 1"

  # Cho ph√©p ch·∫°y manual
  workflow_dispatch:
    inputs:
      start_page:
        description: "Start page"
        required: false
        default: "1"
      end_page:
        description: "End page"
        required: false
        default: "15"

jobs:
  scrape:
    runs-on: ubuntu-latest

    permissions:
      contents: write # C·∫ßn quy·ªÅn write ƒë·ªÉ commit v√† push

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Chrome
        run: |
          # Install Chrome
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: Setup ChromeDriver
        uses: nanasess/setup-chromedriver@v2

      - name: Verify Chrome and ChromeDriver
        run: |
          google-chrome --version
          chromedriver --version

      - name: Create data directory
        run: mkdir -p data

      - name: Run scraper
        env:
          START_PAGE: ${{ github.event.inputs.start_page || '1' }}
          END_PAGE: ${{ github.event.inputs.end_page || '15' }}
        run: |
          cd src
          python main.py

      - name: Commit and push results
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add data/
          git add -A

          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "ü§ñ Auto scrape: $(date +'%Y-%m-%d %H:%M:%S')"
            git push
          fi

      - name: Notify n8n webhook
        if: success()
        run: |
          # ∆Øu ti√™n l·∫•y file Excel ƒë∆∞·ª£c thay ƒë·ªïi ·ªü commit m·ªõi nh·∫•t
          FILE_PATH=$(git log -1 --name-only --pretty="" -- data/*.xlsx | head -1 || echo "")
          if [ -n "$FILE_PATH" ] && [ -f "$FILE_PATH" ]; then
            FILE_NAME=$(basename "$FILE_PATH")
          else
            # Fallback: l·∫•y file m·ªõi nh·∫•t theo th·ªùi gian s·ª≠a ƒë·ªïi
            # Tr√°nh ch·ªçn nh·∫ßm theo th·ª© t·ª± ch·ªØ c√°i (vd: 08_* tr∆∞·ªõc 09_*)
            FILE_NAME=$(ls -t data/*.xlsx 2>/dev/null | head -1 | xargs basename || echo "")
          fi
          SCRAPE_DATE=$(date +'%Y-%m-%d %H:%M:%S')
          SCRAPE_COUNT=$(ls data/*.xlsx 2>/dev/null | wc -l || echo "0")

          if [ ! -z "$FILE_NAME" ]; then
            echo "üìä Found Excel file: $FILE_NAME"
            
            # G·ªçi n8n webhook v·ªõi th√¥ng tin file
            curl -X POST "${{ secrets.N8N_WEBHOOK_URL }}" \
              -H "Content-Type: application/json" \
              -d '{
                "scrape_date": "'"$SCRAPE_DATE"'",
                "file_name": "'"$FILE_NAME"'",
                "file_count": '"$SCRAPE_COUNT"',
                "status": "success",
                "repository": "XuanKyUEL/tradewheel_scrape",
                "github_view_url": "https://github.com/XuanKyUEL/tradewheel_scrape/blob/main/data/'"$FILE_NAME"'",
                "download_url": "https://github.com/XuanKyUEL/tradewheel_scrape/raw/main/data/'"$FILE_NAME"'"
              }' || echo "‚ö†Ô∏è Failed to notify n8n webhook"
          else
            echo "‚ùå No Excel file found to notify"
          fi
        env:
          N8N_WEBHOOK_URL: ${{ secrets.N8N_WEBHOOK_URL }}

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: tradewheel-data
          path: data/
          retention-days: 90
